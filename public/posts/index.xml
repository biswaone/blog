<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Notes/Rambling of a 0x engineer</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on Notes/Rambling of a 0x engineer</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 03 Oct 2023 13:57:56 +0530</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CSE138: Notes</title>
      <link>http://localhost:1313/posts/cse138-lecture-notes/</link>
      <pubDate>Tue, 03 Oct 2023 13:57:56 +0530</pubDate>
      <guid>http://localhost:1313/posts/cse138-lecture-notes/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;cse138-distributed-systems-l3-partial-orders-total-orders-lamport-clocks-vector-clocks&#34;&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=UqY1tF3zxjc&amp;amp;list=PLNPUF5QyWU8PydLG2cIJrCvnn5I_exhYx&amp;amp;index=3&#34;&gt;CSE138 (Distributed Systems) L3: partial orders, total orders, Lamport clocks, vector clocks&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Happens Before:&lt;/strong&gt;&lt;/p&gt;&#xA;  &lt;figure style=&#34;text-align: center;&#34;&gt;&#xA;  &#x9;&lt;img src=&#34;Happens%20Before%20relationship.png&#34; alt=&#34;Descriptive Alt Text&#34; style=&#34;width: 700px;&#34;&gt;&#xA;  &#x9;&lt;figcaption style=&#34;color: #ccc; font-style: italic; font-size: 0.9em;&#34;&gt;&lt;em&gt;Figure: Happens Before relationship&lt;/em&gt;&lt;/figcaption&gt;&#xA;  &lt;/figure&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;if A and B are 2 events in the same process and A happens before B then A -&amp;gt; B&lt;/li&gt;&#xA;&lt;li&gt;if A is a send and B is a corresponding receive then A -&amp;gt; B&lt;/li&gt;&#xA;&lt;li&gt;if A -&amp;gt; B and C -&amp;gt; B then A -&amp;gt; B (transitive)&lt;/li&gt;&#xA;&lt;li&gt;&lt;img src=&#34;Pasted%20image%2020231014132913.png&#34; alt=&#34;Desktop View&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;How are A and E related: A -&amp;gt; B and B -&amp;gt; E so A -&amp;gt; E&lt;/li&gt;&#xA;&lt;li&gt;How are D and A related: Not related&#xA;&lt;ul&gt;&#xA;&lt;li&gt;What all places we can go from A ?&#xA;&lt;ul&gt;&#xA;&lt;li&gt;we can go to B,E,F,G but not D&lt;/li&gt;&#xA;&lt;li&gt;we say that A and D are concurrent&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;partial-orderpartially-ordered-set&#34;&gt;Partial Order/Partially Ordered Set:&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A set S, together with a binary relation often written as $a \leq b$ (a is related to b this can be the happens before relationship) that lets us compare things in set S and has the following properties&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Reflexivity: $ x \in A, \ a \leq a $&lt;/li&gt;&#xA;&lt;li&gt;Anti-symmetry: for all $a, b \in A$ if $a \leq b$ and $b \leq a$  then $a = b$&lt;/li&gt;&#xA;&lt;li&gt;Transitivity: $a,b,c \in S$ if $a \leq b$ and $b \leq c$ then $a \leq c$&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;We have 8 events ${A, B, C, D, E, F, G, H}$&lt;/li&gt;&#xA;&lt;li&gt;We are checking that whether happens before is actually a partial order&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Transitivity is satisfied&lt;/li&gt;&#xA;&lt;li&gt;Anti symmetry is vacuously true&lt;/li&gt;&#xA;&lt;li&gt;Reflexivity is untrue as $a \leq a$ a happens before a is untrue or doesn&amp;rsquo;t make sense&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;The above shows that happens before is not a partial order or can be said irreflexive partial order&lt;/li&gt;&#xA;&lt;li&gt;Example of partial order&#xA;&lt;ul&gt;&#xA;&lt;li&gt;set containment&lt;/li&gt;&#xA;&lt;li&gt;the subsets of set of ${a,b,c}$ is a partial order&lt;/li&gt;&#xA;&lt;li&gt;${∅,{a},{b},{c},{a,b},{a,c},{b,c},{a,b,c}}$&#xA;&lt;ul&gt;&#xA;&lt;li&gt;${a} \leq {a}$  An element is a subset of itself&lt;/li&gt;&#xA;&lt;li&gt;${a,b} \leq  {a,b}$ and ${a,b} \geq  {a,b}$&lt;/li&gt;&#xA;&lt;li&gt;${a} \subseteq {a,b}$ and ${a,b} \subseteq {a,b,c}$ then ${a} \subseteq {a,c}$&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Elements that are not comparable, for those happens before relation says they are concurrent, those are the elements not ordered by partial order&lt;/li&gt;&#xA;&lt;li&gt;In total order every pair of event is ordered&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;clocks&#34;&gt;Clocks&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Physical Clocks&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Time of day clocks&lt;/li&gt;&#xA;&lt;li&gt;monotonic clocks&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Logical Clocks: Ordering of events&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Lamport clocks&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Assigning number to events&lt;/li&gt;&#xA;&lt;li&gt;Denoted as LC(A): Lamport clock of event A&lt;/li&gt;&#xA;&lt;li&gt;clock condition: if $a \to b$  then $LC(A) \leq LC(B)$&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Lamport clocks are consistent with causality&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Lamport clock algorithm&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Every process has to keep a counter initialized to 0&lt;/li&gt;&#xA;&lt;li&gt;On every event on a process that counter has to increment by 1&lt;/li&gt;&#xA;&lt;li&gt;When you send a message, a process needs to include its current counter in the message&lt;/li&gt;&#xA;&lt;li&gt;When receiving a message, a process sets its counter to the max(local-counter, received-counter) + 1&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;img src=&#34;Pasted%20image%2020231014185622.png&#34; alt=&#34;Desktop View&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;if $a \to b$  then $LC(A) \leq LC(B)$ but the reverse is not true&lt;/li&gt;&#xA;&lt;li&gt;&lt;img src=&#34;Pasted%20image%2020231014191504.png&#34; alt=&#34;Desktop View&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Lamport clocks are consistent with (potential) causality&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A -&amp;gt; B then logical clock of A &amp;lt; Logical clock of B&lt;/li&gt;&#xA;&lt;li&gt;characterizes causality&#xA;&lt;ul&gt;&#xA;&lt;li&gt;if LC(A) &amp;lt; LC(B) then A -&amp;gt; B Lamport clocks do not have this property&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;What something we can do with Lamport clocks ?&#xA;&lt;ul&gt;&#xA;&lt;li&gt;what can we do with P -&amp;gt; Q&lt;/li&gt;&#xA;&lt;li&gt;we can take its contrapositive&lt;/li&gt;&#xA;&lt;li&gt;if P implies Q then&#xA;&lt;ul&gt;&#xA;&lt;li&gt;not Q implies not P or $\neg Q \Rightarrow \neg P$&lt;/li&gt;&#xA;&lt;li&gt;$A \rightarrow B \ \Rightarrow \quad LC(A) &amp;lt; LC(B)$&lt;/li&gt;&#xA;&lt;li&gt;$\neg \ (LC(A) &amp;lt; LC(B)) \Rightarrow \neg \ (A \to B)$&lt;/li&gt;&#xA;&lt;li&gt;We can rule out the possibility that A happens before B&lt;/li&gt;&#xA;&lt;li&gt;It might be the case that $ B \rightarrow A \ or \ A \parallel B $&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Lamport clocks can help in ruling out things&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;vector-clocks&#34;&gt;Vector Clocks&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;$A \rightarrow B \ \Rightarrow  LC(A) &amp;lt; LC(B)$  (Lamport clocks)&lt;/li&gt;&#xA;&lt;li&gt;LC are &amp;ldquo;consistent with causality&amp;rdquo;&lt;/li&gt;&#xA;&lt;li&gt;The other direction (&amp;ldquo;characterizes causality&amp;rdquo;) doesn&amp;rsquo;t hold&lt;/li&gt;&#xA;&lt;li&gt;Clocks that is consistent with causality and characterizes causality&lt;/li&gt;&#xA;&lt;li&gt;$A \rightarrow B \ \Leftrightarrow LC(A) &amp;lt; LC(B)$&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Every process keeps a vector (length N for N processes) of integers initialized to 0 $[0,0,0]$&lt;/li&gt;&#xA;&lt;li&gt;On every event, a process increments its own vector clock(all events: sends, receive and internal events)&lt;/li&gt;&#xA;&lt;li&gt;when sending a message, a process includes its current vector clock (after the increment from step 2, because sends are events)&lt;/li&gt;&#xA;&lt;li&gt;when receiving a message, a process will update its vector clock to the maximum(local,received). local is its own vector clock after incrementing its position, because receives are events&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Max of vectors&#xA;&lt;ul&gt;&#xA;&lt;li&gt;$[1,12,4]$ and $[7,0,2]$ will be $[7,12,4]$  (pointwise maximum)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Suppose we have a vector clock of $[5,0,0]$ for alice, bob and carol this means alice has seen 5 events whereas bob and carol haven&amp;rsquo;t seen any&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;cse138-distributed-systems-l4-vector-clocks-fifocausaltotally-ordered-delivery&#34;&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=5BHizc7BPyE&#34;&gt;CSE138 (Distributed Systems) L4: vector clocks, FIFO/causal/totally-ordered delivery&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;img src=&#34;20231015131523.png&#34; alt=&#34;Desktop View&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;All the elements in the red circle constitute the causal history of A&lt;/li&gt;&#xA;&lt;li&gt;Their vector clocks are smaller than A (the value of elements are smaller in every position)&lt;/li&gt;&#xA;&lt;li&gt;The events in the blue circle are the events that are concurrent or causally independent with A&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;protocols&#34;&gt;Protocols&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A set of rules that processes use to communicate with each other&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;correctness-property-of-execution&#34;&gt;correctness property of execution&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;FIFO Delivery: if a process sends message m2 after m1, any process delivering both delivers m1 first and then m2&#xA;&lt;ul&gt;&#xA;&lt;li&gt;sending a message is something you do&lt;/li&gt;&#xA;&lt;li&gt;Receiving a message is something that happens to you&lt;/li&gt;&#xA;&lt;li&gt;Delivering a message is something you can do with a message you receive (you queue up a received messages and wait to deliver them)&lt;/li&gt;&#xA;&lt;li&gt;Violation of FIFO delivery&lt;/li&gt;&#xA;&lt;li&gt;&lt;img src=&#34;Pasted%20image%2020231015190423.png&#34; alt=&#34;Desktop View&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;What can we do to implement FIFO delivery&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Sequence numbers: Tag your messages with sender id and sender sequence number&lt;/li&gt;&#xA;&lt;li&gt;senders increment there sequence number after sending the message&lt;/li&gt;&#xA;&lt;li&gt;if a received message sequence number is the SN of the prev message from that sender + 1, deliver that message&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;What will happen if a message doesn&amp;rsquo;t gets delivered&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;img src=&#34;Pasted%20image%2020231015190818.png&#34; alt=&#34;Desktop View&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;using sequence numbers only works well if you have reliable delivery&lt;/li&gt;&#xA;&lt;li&gt;TCP has reliable delivery&#xA;&lt;ul&gt;&#xA;&lt;li&gt;suppose we don&amp;rsquo;t have reliable delivery&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;img src=&#34;Pasted%20image%2020231015191356.png&#34; alt=&#34;Desktop View&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Vacuously satisfies FIFO delivery&lt;/li&gt;&#xA;&lt;li&gt;After every message receive bob can send ack, this also guarantees FIFO delivery but is also slow and ack could also get lost&lt;/li&gt;&#xA;&lt;li&gt;&lt;img src=&#34;Pasted%20image%2020231015191839.png&#34; alt=&#34;Desktop View&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;causal-delivery&#34;&gt;Causal Delivery&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;if m1 send happened before m2 send then m1&amp;rsquo;s delivery must happen before m2&amp;rsquo;s delivery&lt;/li&gt;&#xA;&lt;li&gt;&lt;img src=&#34;Pasted%20image%2020231015193400.png&#34; alt=&#34;Desktop View&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;How to enforce causal delivery? (Hint: Using vector clocks)&lt;/li&gt;&#xA;&lt;li&gt;Correctness property of execution&#xA;&lt;ul&gt;&#xA;&lt;li&gt;FIFO delivery&lt;/li&gt;&#xA;&lt;li&gt;causal delivery&lt;/li&gt;&#xA;&lt;li&gt;Totally ordered delivery&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;totally-ordered-delivery&#34;&gt;Totally Ordered Delivery&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;if a process delivers m1 and then m2, then all processes delivering both m1 and m2 deliver m1 first and then m2&lt;/li&gt;&#xA;&lt;li&gt;&lt;img src=&#34;Pasted%20image%2020231015194417.png&#34; alt=&#34;Desktop View&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Replication</title>
      <link>http://localhost:1313/posts/ddia-chapter-5-notes/</link>
      <pubDate>Tue, 03 Oct 2023 13:57:56 +0530</pubDate>
      <guid>http://localhost:1313/posts/ddia-chapter-5-notes/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Replication: Keeping a copy of same data&lt;/li&gt;&#xA;&lt;li&gt;why replication  ?&#xA;&lt;ul&gt;&#xA;&lt;li&gt;to keep data geographically close to users -&amp;gt; reduce latency&lt;/li&gt;&#xA;&lt;li&gt;To allow system to continue working even if some parts have failed -&amp;gt; High availability&lt;/li&gt;&#xA;&lt;li&gt;Increase read throughput&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Easy Part: Data does not change over time&#xA;&lt;ul&gt;&#xA;&lt;li&gt;copy data to every node once and you are done&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Hard Part: Handling changes to replicated data (Point of Discussion in this chapter)&lt;/li&gt;&#xA;&lt;li&gt;Algorithms of replicating changes between nodes&#xA;&lt;ul&gt;&#xA;&lt;li&gt;single leader replication&lt;/li&gt;&#xA;&lt;li&gt;multi leader replication&lt;/li&gt;&#xA;&lt;li&gt;leaderless replication&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Trade-offs to consider&#xA;&lt;ul&gt;&#xA;&lt;li&gt;synchronous or asynchronous replication&lt;/li&gt;&#xA;&lt;li&gt;How to handle failed replicas&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;leader-based-replication&#34;&gt;Leader Based Replication&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;One replica is designated leader (master or primary)&lt;/li&gt;&#xA;&lt;li&gt;client writes to the master then the changes are send to replicas as replication log, each follower takes the logs and updates its local copy of the database by applying all the writes in the same order as they were as they were processed on the leader&lt;/li&gt;&#xA;&lt;li&gt;Writes only go through leader, reads can go to any replicas&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;synchronous-versus-asynchronous-replication&#34;&gt;Synchronous Versus asynchronous Replication&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;img src=&#34;Leader-based%20replication%20with%20one%20synchronous%20and%20one%20asynchronous%20fol%E2%80%90lower.png&#34; alt=&#34;Desktop View&#34; title=&#34;Leader Based replication across one synchronous and one asychronous follower&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Replication to follower 1 -&amp;gt; synchronous&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Leader waits until follower 1 confirms it has received the writes before reporting success to user&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Replication to follower 2 -&amp;gt; asynchronous&#xA;&lt;ul&gt;&#xA;&lt;li&gt;the leader sends the message, but doesn’t wait for a response from the follower&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Normally replication is quite fast but scenarios may arise where followers might fall behind followers by several minutes&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Network failures&lt;/li&gt;&#xA;&lt;li&gt;Recovering from failures&lt;/li&gt;&#xA;&lt;li&gt;System operating at maximum capacity&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Advantages of Synchronous Replication&#xA;&lt;ul&gt;&#xA;&lt;li&gt;follower is guaranteed to have an up-to-date copy of the data that is consistent with the leader&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Disadvantages of Synchronous Replication&#xA;&lt;ul&gt;&#xA;&lt;li&gt;If the Synchronous follower doesn&amp;rsquo;t respond the leader has to block all writes until Synchronous replica is available again&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;it is impractical for all followers to be synchronous: any one node outage would cause the whole system to grind to a halt&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;semi-synchronous:&lt;/em&gt; one of the followers is synchronous, and the others are asynchronous&#xA;&lt;ul&gt;&#xA;&lt;li&gt;if synchronous becomes unavailable, asynchronous is made sync and this guarantees we always have copies of data on at least 2 nodes.&lt;/li&gt;&#xA;&lt;li&gt;Durability guarantees in async replication&#xA;&lt;ul&gt;&#xA;&lt;li&gt;If the leader fails and writes have not been replicated to followers, the write is lost&lt;/li&gt;&#xA;&lt;li&gt;But a full async replication has the advantage that the leader can continue processing data even if all of its followers have fallen behind.&lt;/li&gt;&#xA;&lt;li&gt;Here the trade-off is weakening of durability, nevertheless fully async replication is widely used if there are many followers that are geographically distributed&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;chain replication is a variant of synchronous replication that has been successfully implemented in a few systems such as Microsoft Azure Storage&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;setting-up-new-followers&#34;&gt;Setting up new followers&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;you may need to increase the number of replicas or replace failed node, how to ensure that the new node has updated data&lt;/li&gt;&#xA;&lt;li&gt;How to do it ?&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Take a consistent snapshot leader’s database at some point in time&lt;/li&gt;&#xA;&lt;li&gt;Copy the snapshot to the new follower node&lt;/li&gt;&#xA;&lt;li&gt;connects to the leader and requests all the data changes that have happened since the snapshot was taken&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;log sequence number(PostgreSQL)/binlog coordinates(MySQL)&lt;/em&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;the snapshot is associated with an exact position in the leader’s &lt;strong&gt;replication log&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Once follower has processed the backlog of data changes since the snapshot,we say it has caught up&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;handling-node-outages&#34;&gt;Handling Node Outages&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;How do you achieve high availability with leader-based replication?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reliable, Scalable, and Maintainable Applications</title>
      <link>http://localhost:1313/posts/ddia-chapter-1-notes/</link>
      <pubDate>Sun, 22 May 2022 13:57:56 +0530</pubDate>
      <guid>http://localhost:1313/posts/ddia-chapter-1-notes/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Standard building blocks of data intensive applications&#xA;databases&#xA;caches&#xA;search indexes&#xA;stream processing&#xA;batch processing&#xA;The above sounds painfully obvious ?&#xA;because data systems are successful abstractions&#xA;API abstracts away the data system&lt;/li&gt;&#xA;&lt;li&gt;Focus on 3 concerns in software systems&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;em&gt;&lt;strong&gt;Reliability&lt;/strong&gt;&lt;/em&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The systems should continue to work &lt;em&gt;correctly&lt;/em&gt; even in face of adversity&lt;/li&gt;&#xA;&lt;li&gt;Tolerate user mistakes&lt;/li&gt;&#xA;&lt;li&gt;Performance good enough under expected load and volume&lt;/li&gt;&#xA;&lt;li&gt;Prevents unauthorized access and abuse&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;&lt;strong&gt;&amp;ldquo;Continuing to work correctly even when things go wrong&amp;rdquo;&lt;/strong&gt;&lt;/em&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Fault tolerant/ Resilient&lt;/li&gt;&#xA;&lt;li&gt;Things that can go wrong are called fault&lt;/li&gt;&#xA;&lt;li&gt;System that can anticipate fault and deal with it are called fault tolerant&lt;/li&gt;&#xA;&lt;li&gt;Difference between fault and failure&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Fault a single component deviating from spec&lt;/li&gt;&#xA;&lt;li&gt;Failure when the system as a whole fails providing the desired service&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Multiple faults lead to failure of the service&lt;/li&gt;&#xA;&lt;li&gt;Chaos Monkey (Netflix)&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Intentionally Inducing faults to check fault tolerance machinery in action&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Types of Faults&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Hardware Faults&lt;/li&gt;&#xA;&lt;li&gt;Software Faults&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Allowing processes to crash and restart&lt;/li&gt;&#xA;&lt;li&gt;Careful thinking about assumptions and interactions&lt;/li&gt;&#xA;&lt;li&gt;Measuring, monitoring and analyzing&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Human Errors&#xA;&lt;ul&gt;&#xA;&lt;li&gt;How to make systems reliable in spite of unreliable humans&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Allow quick and easy recovery from human errors&lt;/li&gt;&#xA;&lt;li&gt;Quick rollback of configuration changes&lt;/li&gt;&#xA;&lt;li&gt;Roll out code slowly&lt;/li&gt;&#xA;&lt;li&gt;clear monitoring and performance metrics&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;&lt;strong&gt;Scalability&lt;/strong&gt;&lt;/em&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;If a system grows in a particular way what are are ways of dealing with the growth?&lt;/li&gt;&#xA;&lt;li&gt;How can we add computing resources to handle the additional growth&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Load Parameters (Best choice depends on the architecture)&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Request Per Seconds&lt;/li&gt;&#xA;&lt;li&gt;Ratio of reads to write to database&lt;/li&gt;&#xA;&lt;li&gt;No. of simultaneously active users in a chatroom&lt;/li&gt;&#xA;&lt;li&gt;The hit rate on a cache&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Describing load -&amp;gt; Describing Performance&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Increase load parameter and keep system resources unchanged how is the performance of the system affected?&lt;/li&gt;&#xA;&lt;li&gt;When you increase load parameter, how much do you need to increase the resources to keep the performance unchanged ?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Batch Processing System&#xA;&lt;ul&gt;&#xA;&lt;li&gt;We care about throughput, the number of jobs we can process per second or the total time it takes to run a job&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Online Systems&#xA;&lt;ul&gt;&#xA;&lt;li&gt;We care about response time&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Response time: This is what client sees&lt;/li&gt;&#xA;&lt;li&gt;Latency: the duration that a request is waiting to be handled during which it is latent, awaiting service&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;&lt;strong&gt;Response Percentiles&lt;/strong&gt;&lt;/em&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Median&lt;/strong&gt;: If median response time is 200 ms then half your requests return in less than 200 ms, and half your requests take longer than that&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Metric to understand how long typically users have to wait&lt;/li&gt;&#xA;&lt;li&gt;95th, 99th, and 99.9th percentiles&#xA;&lt;ul&gt;&#xA;&lt;li&gt;if the 95th percentile response time is 1.5 seconds, that means 95 out of 100 requests take less than 1.5 seconds, and 5 out of 100 requests take 1.5 seconds or more&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;&lt;strong&gt;Tail Latencies&lt;/strong&gt;&lt;/em&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Directly affect user experience&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Amazon response time requirements for internal services in terms of the 99.9th percentile even though it only affects 1 in 1,000 requests&lt;/li&gt;&#xA;&lt;li&gt;customers with the slowest requests are often those who have the most data on their accounts because they have made many purchase that is, they’re the most valuable customers&lt;/li&gt;&#xA;&lt;li&gt;Optimizing the 99.99th percentile (the slowest 1 in 10,000 requests) was deemed too expensive and to not yield enough benefit for Amazon’s purposes&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;&lt;strong&gt;Head of Line Blocking&lt;/strong&gt;&lt;/em&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Only takes a small number of slow requests to hold up the processing of subsequent requests Even if those subsequent requests are fast to process on the server, the client will see a slow overall response time due to the time waiting for the prior request to complete, thats why necessary to measure response on client side&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;&lt;strong&gt;Tail Latency Amplification&lt;/strong&gt;&lt;/em&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;If end user requires multiple backend calls to serve a request a single slow call can make the entire end user request slow&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;&lt;strong&gt;Approaches For Dealing With Load&lt;/strong&gt;&lt;/em&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Easy: distributing stateless services across multiple machines&lt;/li&gt;&#xA;&lt;li&gt;Hard: taking stateful data systems from a single node to a distributed setup&lt;/li&gt;&#xA;&lt;li&gt;In an early-stage startup or an unproven product it’s usually more important to be able to iterate quickly on product features than it is to scale to some hypothetical future load&lt;/li&gt;&#xA;&lt;li&gt;Architecture is built around certain assumptions (load parameters) and if they are wrong the scaling efforts are wasted.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;&lt;strong&gt;Maintainability&lt;/strong&gt;&lt;/em&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Operability&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Make it easy for operations teams to keep the system running smoothly.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Good operations can often work around the limitations of bad (or incomplete) software, but good software cannot run reliably with bad operations&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Simplicity (Managing complexity)&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Make it easy for new engineers to understand the system&lt;/li&gt;&#xA;&lt;li&gt;when the system is harder for developers to understand and reason about&#xA;&lt;ul&gt;&#xA;&lt;li&gt;hidden assumptions&lt;/li&gt;&#xA;&lt;li&gt;unintended consequences&lt;/li&gt;&#xA;&lt;li&gt;unexpected interactions&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Making a system simpler does not necessarily mean reducing its functionality; it can also mean removing accidental complexity&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Accidental Complexity&#xA;&lt;ul&gt;&#xA;&lt;li&gt;complexity not inherent in the problem that the software solves (as seen by the users) but arises only from the implementation.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Removing Complexity&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Good abstraction&#xA;&lt;ul&gt;&#xA;&lt;li&gt;hide a great deal of implementation detail behind a clean,simple-to-understand façade&lt;/li&gt;&#xA;&lt;li&gt;finding good abstractions is very hard&lt;/li&gt;&#xA;&lt;li&gt;In distributed systems many good algorithms but much less clear how to package them into algorithms that help in managing the Complexity of the system&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;reusable components&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Evolvability&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Make it easy for engineers to make changes to the system in the future&lt;/li&gt;&#xA;&lt;li&gt;agility on a data system level: evolvability&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Trouble with distributed systems</title>
      <link>http://localhost:1313/posts/ddia-chapter-8-notes/</link>
      <pubDate>Sun, 22 May 2022 13:57:56 +0530</pubDate>
      <guid>http://localhost:1313/posts/ddia-chapter-8-notes/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;anything that can go wrong will go wrong&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;This chapter is a thoroughly pessimistic and depressing overview of things that may go wrong in a distributed system&lt;/li&gt;&#xA;&lt;li&gt;working with distributed systems vs writing software for on a single computer&#xA;&lt;ul&gt;&#xA;&lt;li&gt;problems with networks&lt;/li&gt;&#xA;&lt;li&gt;clocks and timing issues&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;faults-and-partial-failures&#34;&gt;Faults and Partial Failures&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;An individual computer with good software is usually either fully functional or entirely broken, but not something in between&lt;/li&gt;&#xA;&lt;li&gt;when the hardware is working correctly, the same operation always produces the same result (it is deterministic)&lt;/li&gt;&#xA;&lt;li&gt;The design goal of computer is always correct computation. Why so?&#xA;&lt;ul&gt;&#xA;&lt;li&gt;if an internal fault occurs like (e.g., memory corruption or a loose connector), we prefer a computer to crash completely rather than returning a wrong result because wrong results are difficult and confusing to deal with&lt;/li&gt;&#xA;&lt;li&gt;computers hide the fuzzy physical reality on which they are implemented and present an idealized system mode&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;writing software that runs on several computers, connected by a network, the situation is fundamentally different&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;Partial Failure:&lt;/em&gt; In a distributed system, there may well be some parts of the system that are broken in some unpredictable way, even though other parts of the system are working fine&lt;/li&gt;&#xA;&lt;li&gt;The difficulty is that partial failures are &lt;em&gt;nondeterministic&lt;/em&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;if you try to do anything involving multiple nodes and the network, it may sometimes work and sometimes unpredictably fail&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;This &lt;em&gt;nondeterminism&lt;/em&gt; and possibility of &lt;em&gt;partial failures&lt;/em&gt; is what makes distributed systems hard to work with&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;cloud-computing-and-supercomputing&#34;&gt;Cloud Computing and Supercomputing&lt;/h3&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3&gt;HPC vs Cloud Computing&lt;/h3&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;    &lt;tr&gt;&#xA;      &lt;th&gt;High-Performance Computing&lt;/th&gt;&#xA;      &lt;th&gt;Cloud Computing&lt;/th&gt;&#xA;    &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;    &lt;tr&gt;&#xA;      &lt;td&gt;Supercomputers with thousands of CPUs for scientific tasks&lt;/td&gt;&#xA;      &lt;td&gt;Multi-tenant datacenters shared by numerous organizations&lt;/td&gt;&#xA;    &lt;/tr&gt;&#xA;    &lt;tr&gt;&#xA;      &lt;td&gt;Checkpointing to restart computations from last checkpoint&lt;/td&gt;&#xA;      &lt;td&gt;Commodity computers connected via IP network&lt;/td&gt;&#xA;    &lt;/tr&gt;&#xA;    &lt;tr&gt;&#xA;      &lt;td&gt;Deals with partial failure by escalating into total failure&lt;/td&gt;&#xA;      &lt;td&gt;Elastic/on-demand resource allocation and uninterrupted service for internet-related applications&lt;/td&gt;&#xA;    &lt;/tr&gt;&#xA;    &lt;tr&gt;&#xA;      &lt;td&gt;Typically built from specialized hardware, where each node is quite reliable&lt;/td&gt;&#xA;      &lt;td&gt;Made using commodity hardware which provides equivalent performance but has a higher rate of failure&lt;/td&gt;&#xA;    &lt;/tr&gt;&#xA;    &lt;tr&gt;&#xA;      &lt;td&gt;Supercomputers often use specialized network topologies, such as multi-dimensional meshes and toruses, which yield better performance&lt;/td&gt;&#xA;      &lt;td&gt;Datacenter networks are often based on IP and Ethernet, arranged in Clos topologies to provide high bisection bandwidth&lt;/td&gt;&#xA;    &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The bigger a system gets, the more likely it is that one of its components is broken. In a system with thousands of nodes, it is reasonable to assume that something is always broken&lt;/li&gt;&#xA;&lt;li&gt;If the system can tolerate failed nodes and still keep working as a whole, that is a very useful feature for operations and maintenance&lt;/li&gt;&#xA;&lt;li&gt;If we want to make distributed systems work, we must &lt;em&gt;accept the possibility of partial failure&lt;/em&gt; and build fault-tolerance mechanisms into the software&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;build a reliable system from unreliable components&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;unreliable-networks&#34;&gt;Unreliable Networks&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;shared-nothing systems:&lt;/em&gt; a bunch of machines connected by a network. The network is the only way those machines can communicate&lt;/li&gt;&#xA;&lt;li&gt;The internet and most internal networks in datacenters (often Ethernet) are &lt;em&gt;asynchronous packet networks&lt;/em&gt;. In this kind of network, one node can send a message (a packet) to another node, but the network gives no guarantees&lt;/li&gt;&#xA;&lt;li&gt;Things can that go wrong?&#xA;&lt;ul&gt;&#xA;&lt;li&gt;request may have been lost (someone unplugged a network cable)&lt;/li&gt;&#xA;&lt;li&gt;request may be waiting in a queue and will be delivered later (network or recipient are overloaded)&lt;/li&gt;&#xA;&lt;li&gt;remote node failure (crash or powered down)&lt;/li&gt;&#xA;&lt;li&gt;garbage collection pauses&lt;/li&gt;&#xA;&lt;li&gt;request processed by remote node but response lost in network/response has been delayed (network or own machine overloaded)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;The sender can’t even tell whether the packet was delivered: the only option is for the recipient to send a response message, which may in turn be lost or delayed&lt;/li&gt;&#xA;&lt;li&gt;The usual way of handling this issue is a &lt;em&gt;timeout&lt;/em&gt;: after some time you give up waiting and assume that the response is not going to arrive. But in this case also we don&amp;rsquo;t know whether the remote host got the request or not.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;network-faults-in-practice&#34;&gt;Network Faults in Practice&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;systemic studies indicate network faults can be can be surprisingly common even in controlled environments like datacenters&lt;/li&gt;&#xA;&lt;li&gt;studies of Network Faults in datacenters&#xA;&lt;ul&gt;&#xA;&lt;li&gt;~12 network faults per month in a mid-sized datacenter of which half disconnected a single machine, and half disconnected an entire rack&lt;/li&gt;&#xA;&lt;li&gt;On studying the failure rates of of components like top-of-rack switches, aggregation switches, and load balancers It found that adding redundant networking gear doesn’t reduce faults as much as you might hope, since it doesn’t guard against human error&lt;/li&gt;&#xA;&lt;li&gt;faults can occur during a software upgrade for a switch could trigger a network topology reconfiguration, during which network packets could be delayed for more than a minute&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Even if network faults are rare in your environment, the fact that faults can occur means that your &lt;em&gt;software needs to be able to handle them&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Handling network faults doesn’t necessarily mean tolerating them&#xA;&lt;ul&gt;&#xA;&lt;li&gt;you do need to know how your software reacts to network problems and ensure that the system can recover from them&lt;/li&gt;&#xA;&lt;li&gt;Chaos Monkey: deliberately trigger network problems and test the system’s response&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;detecting-faults&#34;&gt;Detecting Faults&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;systems need to automatically detect faults&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A load balancer needs to stop sending requests to a dead node&lt;/li&gt;&#xA;&lt;li&gt;In a single leader database if the leader fails then one of the followers needs to be promoted to leader&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;How can we tell if a node is dead ?&#xA;&lt;ul&gt;&#xA;&lt;li&gt;if the machine is reachable but the process crashed then os will refuse connection on that port&lt;/li&gt;&#xA;&lt;li&gt;if the process crashed but the the os is running then a script can notify other nodes about the crash so that another node can takeover quickly&lt;/li&gt;&#xA;&lt;li&gt;Detecting link failure at hardware level needs access to management interface of network switches. If we are connecting via internet or in a shared datacenter that option is gone&lt;/li&gt;&#xA;&lt;li&gt;If a router is sure that the IP address you’re trying to connect to is unreachable, it may reply to you with an ICMP Destination Unreachable packet&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Even if the TCP acknowledges that the packet was delivered there is no guarantee that the application would have handled it before crashing. To know whether an request succeeded the application needs to send a positive response&lt;/li&gt;&#xA;&lt;li&gt;if something has gone wrong we can retry(TCP retries or application level retries) a few times, wait for a timeout to elapse and declare the node dead&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;timeouts-and-unbounded-delays&#34;&gt;Timeouts and Unbounded Delays&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;If a timeout is the only sure way of detecting a fault, then how long should the time‐ out be?&lt;/li&gt;&#xA;&lt;li&gt;long time out: a long wait until a node is declared dead and during this time, users may have to wait or see error messages&lt;/li&gt;&#xA;&lt;li&gt;short time out: detects faults faster, but carries a higher risk of incorrectly declaring a node dead when in fact it has only suffered a temporary slowdown&lt;/li&gt;&#xA;&lt;li&gt;what is a reasonable time out period to use?&#xA;&lt;ul&gt;&#xA;&lt;li&gt;suppose in a system every packet is delivered within $d$ time or it is lost&lt;/li&gt;&#xA;&lt;li&gt;we can assume that a non failed node handles the request in $r$ time&lt;/li&gt;&#xA;&lt;li&gt;a reasonable delay to expect should be $2d + r$ under these guarantees&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Most systems don&amp;rsquo;t operate under such guarantees: &lt;em&gt;asynchronous networks&lt;/em&gt; have unbounded delays(there is no upper time limit on the time it may take for a packet to arrive)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;network-congestion-and-queueing&#34;&gt;Network congestion and queueing&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;queueing at different places&#xA;&lt;ul&gt;&#xA;&lt;li&gt;network switch&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;Network congestion:&lt;/em&gt; when multiple nodes attempt to send packets to the same destination, causing a delay in packet transmission as they are queued up within the network switch.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;operating system&#xA;&lt;ul&gt;&#xA;&lt;li&gt;if all CPU cores are currently busy, the incoming request from the network is queued by the operating system until the application is ready to handle it&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;VM manager&#xA;&lt;ul&gt;&#xA;&lt;li&gt;In virtualized environments, a running operating system is often paused for tens of milliseconds while another virtual machine uses a CPU core. During this time, the VM cannot consume any data from the network, so the incoming data is queued (buffered) by the virtual machine monitor&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Sender&#xA;&lt;ul&gt;&#xA;&lt;li&gt;TCP performs flow control (also known as congestion avoidance or backpressure), in which a node limits its own rate of sending in order to avoid overloading a network link or the receiving node&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;In public clouds and multi-tenant datacenters, resources like network links, switches, and even machine components are shared among many customers. This sharing can lead to variable network delays, especially when batch workloads, such as MapReduce&lt;/li&gt;&#xA;&lt;li&gt;In such scenarios to find the time out period we can measure the distribution of network round-trip times over an extended period, and over many machines, to determine the expected variability of delays&lt;/li&gt;&#xA;&lt;li&gt;rather than using configured constant timeouts, systems can continually measure response times and their variability (jitter), and automatically adjust time‐ outs according to the observed response time distribution. (checkout TCP retransmission timeout it works in the same way)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;synchronous-versus-asynchronous-networks&#34;&gt;Synchronous Versus Asynchronous Networks&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;can we deliver packets with a fixed maximum delay and no packets drop making the systems much simpler?&lt;/li&gt;&#xA;&lt;li&gt;To answer this question, it’s interesting to compare datacenter networks to the traditional fixed-line telephone network(non-cellular, non-VoIP)&#xA;&lt;ul&gt;&#xA;&lt;li&gt;we hardly see any delayed audio frames and dropped calls in telephone network. A phone call requires a constantly low end-to-end latency and enough bandwidth to transfer the audio samples of your voice. It would be nice to have such similar reliability and predictability in computer networks&lt;/li&gt;&#xA;&lt;li&gt;In a call over a telephone network a &lt;em&gt;circuit&lt;/em&gt; is established a fixed, guaranteed amount of bandwidth is allocated for the call, along the entire route between the two callers&lt;/li&gt;&#xA;&lt;li&gt;When a call is established, it is allocated 16 bits of space within each frame (in each direction). Thus, for the duration of the call, each side is guaranteed to be able to send exactly 16 bits of audio data every 250 microseconds&lt;/li&gt;&#xA;&lt;li&gt;This kind of network is &lt;em&gt;synchronous&lt;/em&gt;: even as data passes through several routers, it does not suffer from queueing, because the 16 bits of space for the call have already been reserved in the next hop of the network&lt;/li&gt;&#xA;&lt;li&gt;because there is no queueing, the maximum end-to-end latency of the network is fixed. We call this a &lt;em&gt;bounded delay&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;can-we-not-simply-make-network-delays-predictable&#34;&gt;Can we not simply make network delays predictable?&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A circuit network has fixed reserved bandwidth which no one else can use when the circuit is established, whereas the packets of a TCP connection opportunistically use whatever network bandwidth is available.&lt;/li&gt;&#xA;&lt;li&gt;If datacenter networks and the internet were circuit-switched networks, it would be possible to establish a guaranteed maximum round-trip time when a circuit was set up&lt;/li&gt;&#xA;&lt;li&gt;Ethernet and IP are packet-switched protocols, which suffer from queueing and thus unbounded delays in the network. No concept of circuit&lt;/li&gt;&#xA;&lt;li&gt;Why do datacenter networks and the internet use packet switching?&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The answer is that they are optimized for bursty traffic&lt;/li&gt;&#xA;&lt;li&gt;A circuit is good for an audio or video call, which needs to transfer a fairly constant number of bits per second for the duration of the call&lt;/li&gt;&#xA;&lt;li&gt;if you want to transfer a file over a circuit network, you would have to guess a bandwidth allocation.&lt;/li&gt;&#xA;&lt;li&gt;If you guess too low, the transfer is unnecessarily slow&lt;/li&gt;&#xA;&lt;li&gt;If you guess too high, the circuit cannot be set up (because the net‐ work cannot allow a circuit to be created if its bandwidth allocation cannot be guaranteed)&lt;/li&gt;&#xA;&lt;li&gt;using circuits for bursty data transfers wastes network capacity and makes transfers unnecessarily slow.&lt;/li&gt;&#xA;&lt;li&gt;TCP dynamically adapts the rate of data transfer to the available network capacity.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;unreliable-clocks&#34;&gt;Unreliable Clocks&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;durations vs point in time&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;durations:&lt;/em&gt; request timed out, 99th percentile response time, qps a service handles&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;point-in-time:&lt;/em&gt; when the remainder is sent, when does this cache entry expire&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;monotonic-versus-time-of-day-clocks&#34;&gt;Monotonic Versus Time-of-Day Clocks&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;time-of-day-clocks&#34;&gt;Time-of-day clocks&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;What you intuitively expect a clock to be also known as wall clock&lt;/li&gt;&#xA;&lt;li&gt;Time-of-day clocks are usually synchronized with NTP&lt;/li&gt;&#xA;&lt;li&gt;time-of-day clocks also have various oddities&#xA;&lt;ul&gt;&#xA;&lt;li&gt;if the local clock is too far ahead of the NTP server, it may be forcibly reset and appear to jump back to a previous point in time&lt;/li&gt;&#xA;&lt;li&gt;These jumps, as well as the fact that they often ignore leap seconds, make time-of-day clocks unsuitable for measuring elapsed time&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;monotonic-clocks&#34;&gt;Monotonic clocks&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;measuring a duration (time interval), such as a timeout or a service’s response time&lt;/li&gt;&#xA;&lt;li&gt;check the value of monotonic time at two points and the difference between them tells how much time has elapsed&lt;/li&gt;&#xA;&lt;li&gt;NTP may adjust the frequency at which the monotonic clock moves forward (this is known as slewing the clock) if it detects that the computer’s local quartz is moving faster or slower than the NTP server&lt;/li&gt;&#xA;&lt;li&gt;By default, NTP allows the clock rate to be spee‐ ded up or slowed down by up to 0.05%, but NTP cannot cause the monotonic clock to jump forward or backward.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;In a distributed system, using a monotonic clock for measuring elapsed time (e.g., timeouts) is usually fine, because it doesn’t assume any synchronization between different nodes’ clocks&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;clock-synchronization-and-accuracy&#34;&gt;Clock Synchronization and Accuracy&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Monotonic clocks don’t need synchronization, but time-of-day clocks need to be set according to an NTP server or other external time source in order to be useful.&lt;/li&gt;&#xA;&lt;li&gt;Our methods for getting a clock to tell the correct time isn&amp;rsquo;t nearly as reliable or accurate&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The quartz clock in a computer is not very accurate: it drifts. Google assumes a clock drift of 200 ppm (parts per million) for its servers which is equivalent to 6 ms drift for a clock that is resynchronized with a server every 30 seconds, or 17 seconds drift for a clock that is resynchronized once a day&lt;/li&gt;&#xA;&lt;li&gt;If a computer’s clock differs too much from an NTP server, it may refuse to synchronize, or the local clock will be forcibly reset&lt;/li&gt;&#xA;&lt;li&gt;If a node is accidentally firewalled off from NTP servers, the misconfiguration may go unnoticed for some time.&lt;/li&gt;&#xA;&lt;li&gt;NTP synchronization &lt;em&gt;can only be as good as the network delay&lt;/em&gt;, so there is a limit to its accuracy when you’re on a congested network with variable packet delays.&lt;/li&gt;&#xA;&lt;li&gt;Some NTP servers are wrong or misconfigured, reporting time that is off by hours. NTP clients are quite robust, because they query several servers and ignore outliers&lt;/li&gt;&#xA;&lt;li&gt;Leap seconds result in a minute that is 59 seconds or 61 seconds long, which messes up timing assumptions in systems that are not designed with leap seconds in mind (many major crashes due to leap seconds)&lt;/li&gt;&#xA;&lt;li&gt;In virtual machines, the hardware clock is virtualized, When a CPU core is shared between virtual machines, each VM is paused for tens of milli‐ seconds while another VM is running From an application’s point of view, this pause manifests itself as the clock suddenly jumping forward&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;possible to achieve very good clock accuracy&#xA;&lt;ul&gt;&#xA;&lt;li&gt;MiFID II draft European regulation for financial institutions requires all high-frequency trading funds to synchronize their clocks to within 100 microseconds of UTC&lt;/li&gt;&#xA;&lt;li&gt;Such accuracy can be achieved using GPS receivers, the Precision Time Protocol (PTP), and careful deployment and monitoring&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;relying-on-synchronized-clocks&#34;&gt;Relying on Synchronized Clocks&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;robust software needs to be prepared to deal with incorrect clocks.&lt;/li&gt;&#xA;&lt;li&gt;incorrect clocks easily go unnoticed.&lt;/li&gt;&#xA;&lt;li&gt;if its quartz clock is defective or its NTP client is misconfigured, most things will seem to work fine, even though its clock gradually &lt;em&gt;drifts further and further away from reality&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;if you use software that requires synchronized clocks, it is essential that you also carefully monitor the clock offsets between all the machines.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;timestamps-for-ordering-events&#34;&gt;Timestamps for ordering events&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;if two clients write to a distributed database, who got there first? Which write is the more recent one?&lt;/li&gt;&#xA;&lt;li&gt;Conflicts may happen when time stamps fail to order even correctly. When a write has happened one after the other but their time stamp do not reflect that, the node will incorrectly drop the older value and accept the recent value&lt;/li&gt;&#xA;&lt;li&gt;This conflict resolution strategy is called last write wins (LWW), and it is widely used in both multi-leader replication and leaderless databases such as Cassandra&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Database writes can mysteriously disappear: a node with a lagging clock is unable to overwrite values previously written by a node with a fast clock&lt;/li&gt;&#xA;&lt;li&gt;LWW cannot distinguish between writes that occurred sequentially in quick succession i.e. concurrent writes&lt;/li&gt;&#xA;&lt;li&gt;So-called logical clocks which are based on incrementing counters rather than an oscillating quartz crystal, are a safer alternative for ordering events&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;clock-readings-have-a-confidence-interval&#34;&gt;Clock readings have a confidence interval&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Even if we are able get a very fine grained measurement, that doesn&amp;rsquo;t mean value is accurate to that precision. (drift, network latency,network congestion can make the clock readings variable)&lt;/li&gt;&#xA;&lt;li&gt;it doesn’t make sense to think of a clock reading as a point in time—it is more like a range of times, within a confidence interval: for example, a system may be 95% confident that the time now is between 10.3 and 10.5 seconds past the minute&lt;/li&gt;&#xA;&lt;li&gt;The uncertainty bound can be calculated based on your time source(GPS receiver, atomic clock, uncertainty is based on the expected quartz drift since your last sync with the server, plus the NTP server’s uncertainty, plus the network round-trip time to the server).Unfortunately, most systems don’t expose this uncertainty: for example, when you call clock_gettime(), the return value doesn’t tell you the expected error of the timestamp&lt;/li&gt;&#xA;&lt;li&gt;An interesting exception is Google’s TrueTime API in Spanner, which explicitly reports the confidence interval on the local clock. When you ask it for the current time, you get back two values: &lt;code&gt;[earliest, latest]&lt;/code&gt;, which are the earliest possible and the latest possible timestamp&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;synchronized-clocks-for-global-snapshots&#34;&gt;Synchronized clocks for global snapshots&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The most common implementation of snapshot isolation requires a monotonically increasing transaction ID&lt;/li&gt;&#xA;&lt;li&gt;However, when a database is distributed across many machines, potentially in multiple datacenters, a global, monotonically increasing transaction ID (across all partitions) is difficult to generate&lt;/li&gt;&#xA;&lt;li&gt;Can we use the timestamps from synchronized time-of-day clocks as transaction IDs? If we could get the synchronization good enough, they would have the right properties&lt;/li&gt;&#xA;&lt;li&gt;Google spanner snapshot isolation implementation:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;if you have two confidence intervals, each consisting of an earliest and latest possible timestamp $(A = [A_{earliest}, A_{latest}] \ and \ B = [B_{earliest}, B_{latest}])$, and those two intervals do not overlap $(i.e., A_{earliest} &amp;lt; A_{latest} &amp;lt; B_{earliest} &amp;lt; B_{latest})$, then B definitely happened after A—there can be no doubt. Only if the intervals overlap are we unsure in which order A and B happened&lt;/li&gt;&#xA;&lt;li&gt;In order to ensure that transaction timestamps reflect causality, Spanner deliberately waits for the length of the confidence interval before committing a read-write transaction&lt;/li&gt;&#xA;&lt;li&gt;By doing so, it ensures that any transaction that may read the data is at a sufficiently later time, so their confidence intervals do not overlap.&lt;/li&gt;&#xA;&lt;li&gt;In order to keep the wait time as short as possible, Spanner needs to keep the clock uncertainty as small as possible&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;process-pauses&#34;&gt;Process Pauses&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;a garbage collector (GC) that occasionally needs to stop all running threads. These “stop-the-world” GC pauses have sometimes been known to last for several minutes&lt;/li&gt;&#xA;&lt;li&gt;In virtualized environments, a virtual machine can be suspended (pausing the execution of all processes and saving the contents of memory to disk) and resumed&lt;/li&gt;&#xA;&lt;li&gt;On end-user devices such as laptops, execution may also be suspended and resumed arbitrarily&lt;/li&gt;&#xA;&lt;li&gt;When the operating system context-switches to another thread, or when the hypervisor switches to a different virtual machine (when running in a virtual machine), the currently running thread can be paused at any arbitrary point in the code.In the case of a virtual machine, the CPU time spent in other virtual machines is known as &lt;em&gt;steal time&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;If the application performs synchronous disk access, a thread may be paused waiting for a slow disk I/O operation to complete&lt;/li&gt;&#xA;&lt;li&gt;If the operating system is configured to allow swapping to disk (paging), a simple memory access may result in a page fault that requires a page from disk to be loaded into memory. The thread is paused while this slow I/O operation takes place.&lt;/li&gt;&#xA;&lt;li&gt;All of these occurrences can preempt the running thread at any point and resume it at some later time, without the thread even noticing.&lt;/li&gt;&#xA;&lt;li&gt;A node in a distributed system must assume that its execution can be paused for a significant length of time at any point, even in the middle of a function&lt;/li&gt;&#xA;&lt;li&gt;Limiting the impact of garbage collection&#xA;&lt;ul&gt;&#xA;&lt;li&gt;An emerging idea is to treat GC pauses like brief planned outages of a node, and to let other nodes handle requests from clients while one node is collecting its garbage&lt;/li&gt;&#xA;&lt;li&gt;If the runtime can warn the application that a node soon requires a GC pause, the application can stop sending new requests to that node, wait for it to finish process‐ ing outstanding requests, and then perform the GC while no requests are in progress&lt;/li&gt;&#xA;&lt;li&gt;A variant of this idea is to use the garbage collector only for short-lived objects (which are fast to collect) and to restart processes periodically, before they accumulate enough long-lived objects to require a full GC of long-lived objects&lt;/li&gt;&#xA;&lt;li&gt;One node can be restarted at a time, and traffic can be shifted away from the node before the planned restart, like in a rolling upgrade&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h2 id=&#34;knowledge-truth-and-lies&#34;&gt;Knowledge, Truth, and Lies&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The consequences of these issues(unreliable network with variable delays, and the systems may suffer from partial failures, unreliable clocks, and processing pauses) in distributed systems are profoundly disorienting if you’re not used to distributed systems&lt;/li&gt;&#xA;&lt;li&gt;A node can only find out what state another node is in (what data it has stored, whether it is correctly functioning, etc.) by exchanging messages with it&lt;/li&gt;&#xA;&lt;li&gt;In a distributed system, we can state the assumptions we are making about the behaviour (the system model) and design the actual system in such a way that it meets those assumptions&lt;/li&gt;&#xA;&lt;li&gt;Algorithms can be proved to function correctly within a certain system model.&lt;/li&gt;&#xA;&lt;li&gt;This means that reliable behaviour is achievable, even if the underlying system model provides very few guarantees.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;the-truth-is-defined-by-the-majority&#34;&gt;The Truth Is Defined by the Majority&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;network with an &lt;em&gt;asymmetric fault&lt;/em&gt;: a node is able to receive all messages sent to it, but any outgoing messages from that node are dropped or delayed&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Even though the node is working, other node cannot hear its response and will declare it dead after some time&lt;/li&gt;&#xA;&lt;li&gt;In a slightly better scenario, if the semi disconnected node detects that none of the messages it is sending is getting any response it may and realize that there may be a fault in the network. Nevertheless the node may be declared dead&lt;/li&gt;&#xA;&lt;li&gt;In a third scenario there may be long GC pauses that stops sending and receiving request and response. The other nodes wait and retry and finally declare the node dead.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;The moral of these stories is that a node cannot necessarily trust its own judgment of a situation&lt;/li&gt;&#xA;&lt;li&gt;A distributed system cannot exclusively rely on a single node, because a node may fail at any time, potentially leaving the system stuck and unable to recover&lt;/li&gt;&#xA;&lt;li&gt;many distributed algorithms rely on a &lt;em&gt;quorum&lt;/em&gt;, that is, voting among the nodes&lt;/li&gt;&#xA;&lt;li&gt;If a quorum of nodes declares another node dead, then it must be considered dead, even &lt;em&gt;if that node still very much feels alive&lt;/em&gt; Xd&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;the-leader-and-the-lock&#34;&gt;The leader and the lock&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Frequently, a system requires there to be &lt;em&gt;only one of some thing&lt;/em&gt;?&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Only one node is allowed to be the leader for a database partition, to avoid split brain&lt;/li&gt;&#xA;&lt;li&gt;Only one transaction or client is allowed to hold the lock for a particular resource or object, to prevent concurrently writing to it and corrupting it&lt;/li&gt;&#xA;&lt;li&gt;Only one user is allowed to register a particular username, because a username must uniquely identify a user&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;if a node believes that it is the chosen one(even though majority of the nodes have declared it dead) and continues to behave like it, it could cause problems&lt;/li&gt;&#xA;&lt;li&gt;data corruption bug due to an incorrect implementation of locking&#xA;&lt;ul&gt;&#xA;&lt;li&gt;you want to ensure that a file in a storage service can only be accessed by one client at a time because multiple access can cause the data to be corrupted&lt;/li&gt;&#xA;&lt;li&gt;we implement this by requiring a client to obtain a lease from a lock service before accessing the file&lt;/li&gt;&#xA;&lt;li&gt;if the client holding the lease is paused for too long, its lease expires. Another client can obtain a lease for the same file, and start writing to the file&lt;/li&gt;&#xA;&lt;li&gt;When the paused client comes back, it believes that it still has valid lease and proceeds to write and corrupts the file&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;fencing-tokens&#34;&gt;Fencing tokens&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;We need to ensure that node is not under the false belief that it is the chosen one. A simple technique that achieves this goal is called fencing&lt;/li&gt;&#xA;&lt;li&gt;every time the lock server grants a lock or lease, it also returns a fencing token, which is a number that increases every time a lock is granted&lt;/li&gt;&#xA;&lt;li&gt;We can then require that every time a client sends a write request to the storage service, it must include its current fencing token.&lt;/li&gt;&#xA;&lt;li&gt;When a dead client comes back online and sends its write request to the client along with a fencing token, its write will be rejected if a write with a higher token value has already been processed&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;byzantine-faults&#34;&gt;Byzantine Faults&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Fencing tokens can detect and block a node that is inadvertently acting in error, however if a node acts maliciously and wants to subvert system guarantees it could easily do it by sending fake messages&lt;/li&gt;&#xA;&lt;li&gt;The discussion in the book is limited to the assumption that nodes are unreliable but honest&lt;/li&gt;&#xA;&lt;li&gt;Distributed systems problems become much harder if there is a risk that nodes may “lie” (send arbitrary faulty or corrupted responses)&lt;/li&gt;&#xA;&lt;li&gt;if a node may claim to have received a particular message when in fact it didn’t. Such behaviour is known as a &lt;em&gt;Byzantine fault&lt;/em&gt;, and the problem of reaching consensus in this untrusting environment is known as the &lt;em&gt;Byzantine Generals Problem&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;A system is &lt;em&gt;Byzantine fault-tolerant&lt;/em&gt; if it continues to operate correctly even if some of the &lt;em&gt;nodes are malfunctioning and not obeying the protocol&lt;/em&gt;, or if &lt;em&gt;malicious attackers&lt;/em&gt; are interfering with the network.&lt;/li&gt;&#xA;&lt;li&gt;Situations where Byzantine fault may arise&#xA;&lt;ul&gt;&#xA;&lt;li&gt;In aerospace environments, the data in a computer’s memory or CPU register could become corrupted by radiation leading it to respond to other nodes in arbitrarily unpredictable ways&lt;/li&gt;&#xA;&lt;li&gt;In a system with multiple participating organizations, some participants may attempt to cheat or defraud others. In such circumstances, it is not safe for a node to simply trust another node’s messages. e.g. Bitcoin&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;The discussion here is limited to non byzantine fault tolerant systems&lt;/li&gt;&#xA;&lt;li&gt;A bug in the software could be regarded as a Byzantine fault, but if you deploy the same software to all nodes, then a Byzantine fault-tolerant algorithm cannot save you&lt;/li&gt;&#xA;&lt;li&gt;To use this approach against bugs, you would have to have four independent implementations of the same software and hope that a bug only appears in one of the four implementations&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;system-model-and-reality&#34;&gt;System Model and Reality&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Algorithms need to be written in a way that does not depend too heavily on the details of the hardware and software configuration on which they are run&lt;/li&gt;&#xA;&lt;li&gt;This in turn requires that we somehow formalize the kinds of faults that we expect to happen in a system&lt;/li&gt;&#xA;&lt;li&gt;We do this by defining a &lt;em&gt;system model&lt;/em&gt;, which is an abstraction that describes what &lt;em&gt;things an algorithm may assume&lt;/em&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;System models wrt &lt;em&gt;timing&lt;/em&gt; assumptions&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;Synchronous model:&lt;/em&gt; The synchronous model assumes bounded network delay, bounded process pauses,and bounded clock error. Not a realistic model for most practical systems&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;Partially synchronous model:&lt;/em&gt; Partial synchrony means that a system behaves like a synchronous system most of the time, but it sometimes exceeds the bounds for network delay, process pauses, and clock drift. This is a realistic model of many systems: most of the time, networks and processes are quite well behaved&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;Asynchronous model:&lt;/em&gt; an algorithm is not allowed to make any timing assumptions—in fact, it does not even have a clock (so it cannot use timeouts)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;system model wrt node failure assumptions&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;Crash-stop faults:&lt;/em&gt; Node can stop in only one way i.e. crashing and never come back&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;Crash-recovery faults:&lt;/em&gt; We assume that nodes may crash at any moment, and perhaps start responding again after some unknown time. In this failure model nodes are assumed to have stable storage.&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;Byzantine (arbitrary) faults:&lt;/em&gt; Nodes may do absolutely anything&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;For modeling real systems, the partially synchronous model with crash-recovery faults is generally the most useful model.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;correctness-of-an-algorithm&#34;&gt;Correctness of an algorithm&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;what is means for an algorithm to be correct?&lt;/li&gt;&#xA;&lt;li&gt;we can write down the properties we want of a distributed algorithm to define what it means to be correct&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;safety-and-liveness&#34;&gt;Safety and liveness&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;safety&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Safety is often informally defined as &lt;em&gt;nothing bad happens&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;If a safety property is violated, we can point at a particular point in time at which it was broken&lt;/li&gt;&#xA;&lt;li&gt;After a safety property has been violated, the violation cannot be undone the damage is already done&lt;/li&gt;&#xA;&lt;li&gt;For distributed algorithms, it is common to require that safety properties always hold, in all possible situations of a system model&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;liveness&#xA;&lt;ul&gt;&#xA;&lt;li&gt;it is defined as &lt;em&gt;something good eventually happens&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;liveness properties often include the word “eventually” in their definition&lt;/li&gt;&#xA;&lt;li&gt;it may not hold at some point in time, but there is always hope that it may be satisfied in the future&lt;/li&gt;&#xA;&lt;li&gt;with liveness properties we are allowed to make caveats: for example, we could say that a request needs to receive a response only if a majority of nodes have not crashed&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;mapping-system-models-to-the-real-world&#34;&gt;Mapping system models to the real world&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;When implementing distributed systems messy reality of real world come out. It becomes clear that system model is a simplified abstraction of reality&lt;/li&gt;&#xA;&lt;li&gt;Quorum algorithms rely on node remembering the data, that it claims to have stored. If a node may suffer from amnesia and forget previously stored data, that breaks the quorum condition&lt;/li&gt;&#xA;&lt;li&gt;what happens when assumption gets broken -&amp;gt; correctness of algorithm gets broken&lt;/li&gt;&#xA;&lt;li&gt;Perhaps a new system model? assumption being, storage nodes mostly survive crashes, but sometimes may be lost, This makes reasoning harder&lt;/li&gt;&#xA;&lt;li&gt;The theoretical description of an algorithm can declare certain things correct under certain assumptions, but in real world those assumptions can break and needs to be handled.&lt;/li&gt;&#xA;&lt;li&gt;This doesn&amp;rsquo;t make theoretical models useless, quite the opposite, They are incredibly helpful for distilling down the complexity of real systems to a manageable set of faults that we can reason about&lt;/li&gt;&#xA;&lt;li&gt;We can prove algorithms correct by showing that their properties always hold in some system model.&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;Proving an algorithm correct does not mean its implementation on a real system will necessarily always behave correctly&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;But it’s a very good first step, because the theoretical analysis can uncover problems in an algorithm that might remain hidden for a long time in a real system, and that only come to bite you when your assumptions (e.g., about timing) are defeated due to unusual circumstances&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
